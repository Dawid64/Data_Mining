{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab is to introduce students to the [Snorkel](http://www.snorkel.org) tool and the possibilities of programmatic label generation using the weak-supervised learning paradigm.\n",
    "\n",
    "In order to use weakly supervised learning to generate labels, it is necessary to create three datasets:\n",
    "\n",
    "- **train set**: which does not have any labels\n",
    "- **validation set**: used for hyperparameter optimization, has labels\n",
    "- **test set**: used only for final model evaluation, has labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling functions\n",
    "\n",
    "The first step will be to load the dataset and split it into a train set and a test set. Since in our set all SMS have a label, we will simulate a weakly supervised learning problem by randomly removing 80% of the labels. Additionally, Snorkel requires numeric labels, so we need to recode the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:04.995959Z",
     "start_time": "2021-05-19T19:35:04.879076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "ham\tOk lar... Joking wif u oni...\n",
      "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "ham\tU dun say so early hor... U c already then say...\n",
      "ham\tNah I don't think he goes to usf, he lives around here though\n",
      "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
     ]
    }
   ],
   "source": [
    "!head data/smsspamcollection.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:05.564736Z",
     "start_time": "2021-05-19T19:35:05.335569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          text  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "   label  \n",
       "0     -1  \n",
       "1     -1  \n",
       "2      1  \n",
       "3     -1  \n",
       "4     -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('max_colwidth', 600)\n",
    "\n",
    "SPAM = 1\n",
    "HAM = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "df = pd.read_csv('./data/smsspamcollection.csv', sep='\\t', header=None, names=['old_label', 'text'])\n",
    "\n",
    "df['label'] = df.old_label.apply(lambda x: SPAM if x == 'spam' else HAM)\n",
    "\n",
    "df.loc[df.sample(frac=0.8).index, 'label'] = ABSTAIN\n",
    "df.drop(columns=['old_label'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:05.899853Z",
     "start_time": "2021-05-19T19:35:05.891864Z"
    }
   },
   "outputs": [],
   "source": [
    "abstain_idx = df.label == ABSTAIN\n",
    "\n",
    "df_train = df[abstain_idx]\n",
    "df_test = df[~abstain_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple keyword search\n",
    "\n",
    "As a first example, we will use a search for the words \"check\" and \"free\" in SMS content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:07.160530Z",
     "start_time": "2021-05-19T19:35:06.743723Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def check(sms):\n",
    "    return SPAM if \"check\" in sms.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def free(sms):\n",
    "    return SPAM if \"free\" in sms.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to apply the labeling functions to the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:07.664922Z",
     "start_time": "2021-05-19T19:35:07.537784Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4458/4458 [00:00<00:00, 148728.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [check, free]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of applying the set of labeling functions to the train set is a matrix of size $m \\times n$, where $m$ is the number of examples and $n$ is the number of labeling functions. The matrix contains the result of applying each function to each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:08.394943Z",
     "start_time": "2021-05-19T19:35:08.383414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1],\n",
       "       [-1, -1],\n",
       "       [-1, -1],\n",
       "       ...,\n",
       "       [-1, -1],\n",
       "       [-1,  1],\n",
       "       [-1, -1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:08.802557Z",
     "start_time": "2021-05-19T19:35:08.794050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     Ok lar... Joking wif u oni...\n",
       "label                               -1\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to analyze this is to determine the coverage of labeling functions (i.e., the percentage of cases for which the function returned a result other than `ABSTAIN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:09.658152Z",
     "start_time": "2021-05-19T19:35:09.652780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage for check(): 1.1%\n",
      "Coverage for free(): 4.6%\n"
     ]
    }
   ],
   "source": [
    "coverage_check, coverage_free = (L_train != ABSTAIN).mean(axis=0)\n",
    "\n",
    "print(f\"Coverage for check(): {coverage_check * 100:.1f}%\")\n",
    "print(f\"Coverage for free(): {coverage_free * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, Snorkel offers additional tools that allow for deeper analysis of the result of labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:10.422401Z",
     "start_time": "2021-05-19T19:35:10.392383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       j Polarity  Coverage  Overlaps  Conflicts\n",
       "check  0      [1]  0.011216       0.0        0.0\n",
       "free   1      [1]  0.045536       0.0        0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of each column is as follows:\n",
    "- `Polarity`: the set of labels returned by the function\n",
    "- `Coverage`: the percentage of examples for which the function returns a value other than `ABSTAIN`\n",
    "- Overlaps: the percentage of examples for which at least one other labeling function returned a value\n",
    "- Conflicts: the percentage of examples for which at least one other labeling function returned a different value\n",
    "\n",
    "If the train set contained labels, the method would also return:\n",
    "- `Correct`: the number of correct labels\n",
    "- `Incorrect`: number of incorrect labels\n",
    "- `Empirical Accuracy`: the percentage of correct labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the examples labeled by the `free()` function as spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:11.544719Z",
     "start_time": "2021-05-19T19:35:11.527030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>44 7732584351, Do you want a New Nokia 3510i colour phone DeliveredTomorrow? With 300 free minutes to any mobile + 100 free texts + Free Camcorder reply or call 08000930705.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Are you free now?can i call now?</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>Do you want a New Nokia 3510i Colour Phone Delivered Tomorrow? With 200 FREE minutes to any mobile + 100 FREE text + FREE camcorder Reply or Call 08000930705</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>Thanks for the Vote. Now sing along with the stars with Karaoke on your mobile. For a FREE link just reply with SING now.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4903</th>\n",
       "      <td>* FREE* POLYPHONIC RINGTONE Text SUPER to 87131 to get your FREE POLY TONE of the week now! 16 SN PoBox202 NR31 7ZS subscription 450pw</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>Xmas Offer! Latest Motorola, SonyEricsson &amp; Nokia &amp; FREE Bluetooth or DVD! Double Mins &amp; 1000 Txt on Orange. Call MobileUpd8 on 08000839402 or call2optout/4QF2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>Hi darlin i cantdo anythingtomorrow as myparents aretaking me outfor a meal. when are u free? Katexxx</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>FREE2DAY sexy St George's Day pic of Jordan!Txt PIC to 89080 dont miss out, then every wk a saucy celeb!4 more pics c PocketBabe.co.uk 0870241182716 £3/wk</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>FreeMsg: Fancy a flirt? Reply DATE now &amp; join the UKs fastest growing mobile dating service. Msgs rcvd just 25p to optout txt stop to 83021. Reply DATE now!</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse???</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>Double Mins &amp; 1000 txts on Orange tariffs. Latest Motorola, SonyEricsson &amp; Nokia with Bluetooth FREE! Call MobileUpd8 on 08000839402 or call2optout/HF8</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>December only! Had your mobile 11mths+? You are entitled to update to the latest colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4062</th>\n",
       "      <td>Aight I've been set free, think you could text me blake's address? It occurs to me I'm not quite as sure what I'm doing as I thought I was</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>Hi! You just spoke to MANEESHA V. We'd like to know if you were satisfied with the experience. Reply Toll Free with Yes or No.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4086</th>\n",
       "      <td>Orange brings you ringtones from all time Chart Heroes, with a free hit each week! Go to Ringtones &amp; Pics on wap. To stop receiving these tips reply STOP.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>500 free text msgs. Just text ok to 80488 and we'll credit your account</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>Loan for any purpose £500 - £75,000. Homeowners + Tenants welcome. Have you been previously refused? We can still help. Call Free 0800 1956669 or text back 'help'</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>Call FREEPHONE 0800 542 0578 now!</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>1st wk FREE! Gr8 tones str8 2 u each wk. Txt NOKIA ON to 8007 for Classic Nokia tones or HIT ON to 8007 for Polys. Nokia/150p Poly/200p 16+</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>Free entry in 2 a weekly comp for a chance to win an ipod. Txt POD to 80182 to get entry (std txt rate) T&amp;C's apply 08452810073 for details 18+</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                               text  \\\n",
       "3266  44 7732584351, Do you want a New Nokia 3510i colour phone DeliveredTomorrow? With 300 free minutes to any mobile + 100 free texts + Free Camcorder reply or call 08000930705.   \n",
       "495                                                                                                                                                Are you free now?can i call now?   \n",
       "3595                  Do you want a New Nokia 3510i Colour Phone Delivered Tomorrow? With 200 FREE minutes to any mobile + 100 FREE text + FREE camcorder Reply or Call 08000930705   \n",
       "1507                                                      Thanks for the Vote. Now sing along with the stars with Karaoke on your mobile. For a FREE link just reply with SING now.   \n",
       "4903                                         * FREE* POLYPHONIC RINGTONE Text SUPER to 87131 to get your FREE POLY TONE of the week now! 16 SN PoBox202 NR31 7ZS subscription 450pw   \n",
       "2980                Xmas Offer! Latest Motorola, SonyEricsson & Nokia & FREE Bluetooth or DVD! Double Mins & 1000 Txt on Orange. Call MobileUpd8 on 08000839402 or call2optout/4QF2   \n",
       "3436                                                                          Hi darlin i cantdo anythingtomorrow as myparents aretaking me outfor a meal. when are u free? Katexxx   \n",
       "2109                     FREE2DAY sexy St George's Day pic of Jordan!Txt PIC to 89080 dont miss out, then every wk a saucy celeb!4 more pics c PocketBabe.co.uk 0870241182716 £3/wk   \n",
       "2705                   FreeMsg: Fancy a flirt? Reply DATE now & join the UKs fastest growing mobile dating service. Msgs rcvd just 25p to optout txt stop to 83021. Reply DATE now!   \n",
       "4298                                     thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse???   \n",
       "3891                        Double Mins & 1000 txts on Orange tariffs. Latest Motorola, SonyEricsson & Nokia with Bluetooth FREE! Call MobileUpd8 on 08000839402 or call2optout/HF8   \n",
       "5460                  December only! Had your mobile 11mths+? You are entitled to update to the latest colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906   \n",
       "4062                                     Aight I've been set free, think you could text me blake's address? It occurs to me I'm not quite as sure what I'm doing as I thought I was   \n",
       "2644                                                 Hi! You just spoke to MANEESHA V. We'd like to know if you were satisfied with the experience. Reply Toll Free with Yes or No.   \n",
       "4086                     Orange brings you ringtones from all time Chart Heroes, with a free hit each week! Go to Ringtones & Pics on wap. To stop receiving these tips reply STOP.   \n",
       "1625                                                                                                        500 free text msgs. Just text ok to 80488 and we'll credit your account   \n",
       "3009             Loan for any purpose £500 - £75,000. Homeowners + Tenants welcome. Have you been previously refused? We can still help. Call Free 0800 1956669 or text back 'help'   \n",
       "1777                                                                                                                                              Call FREEPHONE 0800 542 0578 now!   \n",
       "3126                                    1st wk FREE! Gr8 tones str8 2 u each wk. Txt NOKIA ON to 8007 for Classic Nokia tones or HIT ON to 8007 for Polys. Nokia/150p Poly/200p 16+   \n",
       "3123                                Free entry in 2 a weekly comp for a chance to win an ipod. Txt POD to 80182 to get entry (std txt rate) T&C's apply 08452810073 for details 18+   \n",
       "\n",
       "      label  \n",
       "3266     -1  \n",
       "495      -1  \n",
       "3595     -1  \n",
       "1507     -1  \n",
       "4903     -1  \n",
       "2980     -1  \n",
       "3436     -1  \n",
       "2109     -1  \n",
       "2705     -1  \n",
       "4298     -1  \n",
       "3891     -1  \n",
       "5460     -1  \n",
       "4062     -1  \n",
       "2644     -1  \n",
       "4086     -1  \n",
       "1625     -1  \n",
       "3009     -1  \n",
       "1777     -1  \n",
       "3126     -1  \n",
       "3123     -1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[L_train[:,1] == SPAM].sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the phrase \"call now\" is also a good indicator for spam. So let's add one more labeling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:12.390882Z",
     "start_time": "2021-05-19T19:35:12.232917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4458/4458 [00:00<00:00, 54310.89it/s]\n"
     ]
    }
   ],
   "source": [
    "@labeling_function()\n",
    "def call_now(sms):\n",
    "    return SPAM if \"call now\" in sms.text.lower() else ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:12.520535Z",
     "start_time": "2021-05-19T19:35:12.493371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_now</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          j Polarity  Coverage  Overlaps  Conflicts\n",
       "check     0      [1]  0.011216  0.000000        0.0\n",
       "free      1      [1]  0.045536  0.001346        0.0\n",
       "call_now  2      [1]  0.004486  0.001346        0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which examples were labeled as spam by the `call_now()` function but omitted by `free()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:13.266968Z",
     "start_time": "2021-05-19T19:35:13.238111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-1, -1): array([   0,    1,    2, ..., 4454, 4455, 4457]),\n",
       " (1,\n",
       "  -1): array([   9,   71,   77,  113,  141,  143,  150,  176,  203,  224,  244,\n",
       "         272,  298,  301,  326,  355,  385,  409,  445,  460,  472,  499,\n",
       "         521,  534,  580,  612,  616,  628,  629,  637,  680,  744,  786,\n",
       "         797,  842,  897,  917,  960,  964, 1005, 1071, 1119, 1136, 1146,\n",
       "        1171, 1181, 1182, 1193, 1207, 1215, 1238, 1263, 1277, 1291, 1307,\n",
       "        1328, 1377, 1384, 1389, 1392, 1400, 1449, 1492, 1508, 1523, 1549,\n",
       "        1584, 1608, 1633, 1659, 1669, 1690, 1704, 1727, 1757, 1792, 1810,\n",
       "        1820, 1829, 1830, 1865, 1873, 1881, 1933, 2000, 2010, 2053, 2061,\n",
       "        2098, 2111, 2127, 2148, 2162, 2165, 2235, 2271, 2278, 2353, 2375,\n",
       "        2380, 2389, 2396, 2401, 2491, 2494, 2505, 2531, 2607, 2644, 2647,\n",
       "        2658, 2717, 2725, 2728, 2729, 2738, 2743, 2794, 2812, 2867, 2922,\n",
       "        2937, 2950, 2964, 2978, 3005, 3009, 3017, 3019, 3055, 3084, 3085,\n",
       "        3115, 3130, 3136, 3155, 3161, 3179, 3185, 3211, 3214, 3237, 3255,\n",
       "        3268, 3276, 3281, 3320, 3332, 3334, 3337, 3342, 3371, 3389, 3401,\n",
       "        3451, 3454, 3526, 3548, 3554, 3589, 3654, 3679, 3680, 3688, 3699,\n",
       "        3791, 3793, 3904, 3919, 3935, 3938, 3971, 3972, 4012, 4055, 4057,\n",
       "        4059, 4071, 4086, 4090, 4097, 4117, 4121, 4145, 4165, 4225, 4269,\n",
       "        4278, 4299, 4320, 4351, 4366, 4372, 4395, 4432, 4438, 4456]),\n",
       " (1, 1): array([  34,  362,  388,  623, 2150, 3202]),\n",
       " (-1,\n",
       "  1): array([ 685, 1072, 1178, 1711, 2287, 2381, 2524, 3265, 3296, 3441, 3512,\n",
       "        3849, 4373, 4385])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "buckets = get_label_buckets(L_train[:, 1], L_train[:, 2])\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:13.561565Z",
     "start_time": "2021-05-19T19:35:13.544427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>Shop till u Drop, IS IT YOU, either 10K, 5K, £500 Cash or £100 Travel voucher, Call now, 09064011000. NTT PO Box CR01327BT fixedline Cost 150ppm mobile vary</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870..k</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national rate call</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>Shop till u Drop, IS IT YOU, either 10K, 5K, £500 Cash or £100 Travel voucher, Call now, 09064011000. NTT PO Box CR01327BT fixedline Cost 150ppm mobile vary</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>YOUR CHANCE TO BE ON A REALITY FANTASY SHOW call now = 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national = rate call.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national rate call</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870..k</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>Loans for any purpose even if you have Bad Credit! Tenants Welcome. Call NoWorriesLoans.com on 08717111821</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>HOT LIVE FANTASIES call now 08707500020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national rate call</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>U can call now...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>Do you want a new Video handset? 750 any time any network mins? UNLIMITED TEXT? Camcorder? Reply or Call now 08000930705 for del Sat AM</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>URGENT This is our 2nd attempt to contact U. Your £900 prize from YESTERDAY is still awaiting collection. To claim CALL NOW 09061702893</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>URGENT! Last weekend's draw shows that you have won £1000 cash or a Spanish holiday! CALL NOW 09050000332 to claim. T&amp;C: RSTM, SW7 3SS. 150ppm</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5481</th>\n",
       "      <td>Shall call now dear having food</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                              text  \\\n",
       "876   Shop till u Drop, IS IT YOU, either 10K, 5K, £500 Cash or £100 Travel voucher, Call now, 09064011000. NTT PO Box CR01327BT fixedline Cost 150ppm mobile vary   \n",
       "1366                                                         HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870..k   \n",
       "1502                                    HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national rate call   \n",
       "2170  Shop till u Drop, IS IT YOU, either 10K, 5K, £500 Cash or £100 Travel voucher, Call now, 09064011000. NTT PO Box CR01327BT fixedline Cost 150ppm mobile vary   \n",
       "2871      YOUR CHANCE TO BE ON A REALITY FANTASY SHOW call now = 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national = rate call.   \n",
       "2992                                    HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national rate call   \n",
       "3167                                                         HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870..k   \n",
       "4073                                                    Loans for any purpose even if you have Bad Credit! Tenants Welcome. Call NoWorriesLoans.com on 08717111821   \n",
       "4108                                    HOT LIVE FANTASIES call now 08707500020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national rate call   \n",
       "4283                                                                                                                                             U can call now...   \n",
       "4371                       Do you want a new Video handset? 750 any time any network mins? UNLIMITED TEXT? Camcorder? Reply or Call now 08000930705 for del Sat AM   \n",
       "4797                       URGENT This is our 2nd attempt to contact U. Your £900 prize from YESTERDAY is still awaiting collection. To claim CALL NOW 09061702893   \n",
       "5468                URGENT! Last weekend's draw shows that you have won £1000 cash or a Spanish holiday! CALL NOW 09050000332 to claim. T&C: RSTM, SW7 3SS. 150ppm   \n",
       "5481                                                                                                                               Shall call now dear having food   \n",
       "\n",
       "      label  \n",
       "876      -1  \n",
       "1366     -1  \n",
       "1502     -1  \n",
       "2170     -1  \n",
       "2871     -1  \n",
       "2992     -1  \n",
       "3167     -1  \n",
       "4073     -1  \n",
       "4108     -1  \n",
       "4283     -1  \n",
       "4371     -1  \n",
       "4797     -1  \n",
       "5468     -1  \n",
       "5481     -1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[buckets[(ABSTAIN, SPAM)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:14.070016Z",
     "start_time": "2021-05-19T19:35:14.050884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_now</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          j Polarity  Coverage  Overlaps  Conflicts\n",
       "check     0      [1]  0.011216  0.000000        0.0\n",
       "free      1      [1]  0.045536  0.001346        0.0\n",
       "call_now  2      [1]  0.004486  0.001346        0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks as spam all messages containing the word \"HOT\" written in capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:15.054371Z",
     "start_time": "2021-05-19T19:35:15.048930Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def hot(sms):\n",
    "    return SPAM if \"HOT\" in sms.text else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching based on a regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of labeling function is one that uses regexp to find specific expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:16.266072Z",
     "start_time": "2021-05-19T19:35:15.979235Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4458/4458 [00:00<00:00, 67612.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.005384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_now</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_I_am_free</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.005384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 j Polarity  Coverage  Overlaps  Conflicts\n",
       "check            0      [1]  0.011216  0.000000   0.000000\n",
       "free             1      [1]  0.045536  0.045536   0.005384\n",
       "call_now         2      [1]  0.004486  0.001346   0.000000\n",
       "regex_I_am_free  3   [0, 1]  0.045536  0.045536   0.005384"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "@labeling_function()\n",
    "def regex_I_am_free(sms):\n",
    "    if re.search(r\"I\\s.*free\", sms.text, flags=re.I):\n",
    "        return HAM\n",
    "    elif re.search(r\"free\", sms.text, flags=re.I):\n",
    "        return SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now, regex_I_am_free]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare examples that the `free()` function labels as spam and the `regex_I_am_free()` function considers valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:17.020431Z",
     "start_time": "2021-05-19T19:35:16.996219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3692</th>\n",
       "      <td>Sir, i am waiting for your call, once free please call me.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>Do you want a New Nokia 3510i Colour Phone Delivered Tomorrow? With 200 FREE minutes to any mobile + 100 FREE text + FREE camcorder Reply or Call 8000930705</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>Argh my 3g is spotty, anyway the only thing I remember from the research we did was that province and sterling were the only problem-free places we looked at</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>Mila, age23, blonde, new in UK. I look sex with UK guys. if u like fun with me. Text MTALK to 69866.18 . 30pp/txt 1st 5free. £1.50 increments. Help08718728876</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>Hi this is Amy, we will be sending you a free phone number in a couple of days, which will give you an access to all the adult parties...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>I only work from mon to thurs but Sat i cant leh... Booked liao... Which other day u free?</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>Hi darlin i cantdo anythingtomorrow as myparents aretaking me outfor a meal. when are u free? Katexxx</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>I prefer my free days... Tues, wed, fri oso can... Ü ask those workin lor...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>Hi if ur lookin 4 saucy daytime fun wiv busty married woman Am free all next week Chat now 2 sort time 09099726429 JANINExx Calls£1/minMobsmoreLKPOBOX177HP51FL</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4587</th>\n",
       "      <td>Mila, age23, blonde, new in UK. I look sex with UK guys. if u like fun with me. Text MTALK to 69866.18 . 30pp/txt 1st 5free. £1.50 increments. Help08718728876</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                 text  \\\n",
       "3692                                                                                                       Sir, i am waiting for your call, once free please call me.   \n",
       "4386     Do you want a New Nokia 3510i Colour Phone Delivered Tomorrow? With 200 FREE minutes to any mobile + 100 FREE text + FREE camcorder Reply or Call 8000930705   \n",
       "2121    Argh my 3g is spotty, anyway the only thing I remember from the research we did was that province and sterling were the only problem-free places we looked at   \n",
       "3778   Mila, age23, blonde, new in UK. I look sex with UK guys. if u like fun with me. Text MTALK to 69866.18 . 30pp/txt 1st 5free. £1.50 increments. Help08718728876   \n",
       "4949                        Hi this is Amy, we will be sending you a free phone number in a couple of days, which will give you an access to all the adult parties...   \n",
       "4151                                                                       I only work from mon to thurs but Sat i cant leh... Booked liao... Which other day u free?   \n",
       "3436                                                            Hi darlin i cantdo anythingtomorrow as myparents aretaking me outfor a meal. when are u free? Katexxx   \n",
       "2225                                                                                     I prefer my free days... Tues, wed, fri oso can... Ü ask those workin lor...   \n",
       "1663  Hi if ur lookin 4 saucy daytime fun wiv busty married woman Am free all next week Chat now 2 sort time 09099726429 JANINExx Calls£1/minMobsmoreLKPOBOX177HP51FL   \n",
       "4587   Mila, age23, blonde, new in UK. I look sex with UK guys. if u like fun with me. Text MTALK to 69866.18 . 30pp/txt 1st 5free. £1.50 increments. Help08718728876   \n",
       "\n",
       "      label  \n",
       "3692     -1  \n",
       "4386     -1  \n",
       "2121     -1  \n",
       "3778     -1  \n",
       "4949     -1  \n",
       "4151     -1  \n",
       "3436     -1  \n",
       "2225     -1  \n",
       "1663     -1  \n",
       "4587     -1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets = get_label_buckets(L_train[:, 1], L_train[:, 3])\n",
    "df_train.iloc[buckets[(SPAM, HAM)]].sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that will mark as spam all messages containing any amounts specified with a currency symbol ($99, £1.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:18.671866Z",
     "start_time": "2021-05-19T19:35:18.669094Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def contains_money(sms):\n",
    "    return SPAM if re.search(r\"[$£€][0-9]\", sms.text) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching based on heuristics\n",
    "\n",
    "A simple heuristic to find spam is to assume that if more than 10% of the message text is written in capitals, there is a good chance it is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:20.563651Z",
     "start_time": "2021-05-19T19:35:20.207843Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4458/4458 [00:00<00:00, 37386.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.005384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_now</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_I_am_free</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.005384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_many_uppercase_words</th>\n",
       "      <td>4</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.180574</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.001122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains_money</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.048901</td>\n",
       "      <td>0.030283</td>\n",
       "      <td>0.000673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          j Polarity  Coverage  Overlaps  Conflicts\n",
       "check                     0      [1]  0.011216  0.001795   0.000000\n",
       "free                      1      [1]  0.045536  0.045536   0.005384\n",
       "call_now                  2      [1]  0.004486  0.004038   0.000000\n",
       "regex_I_am_free           3   [0, 1]  0.045536  0.045536   0.005384\n",
       "has_many_uppercase_words  4      [1]  0.180574  0.041050   0.001122\n",
       "contains_money            5      [1]  0.048901  0.030283   0.000673"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@labeling_function()\n",
    "def has_many_uppercase_words(sms):\n",
    "    percentage_uppercase = sum([word.isupper() for word in sms.text.split()]) / len(sms.text.split())\n",
    "    \n",
    "    return SPAM if percentage_uppercase > 0.1 else ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words, contains_money]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks as valid those messages that are shorter than 10 words and do not contain any word written in capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:21.341099Z",
     "start_time": "2021-05-19T19:35:21.333785Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def short_and_no_uppercase(sms):\n",
    "    if sms.text.count(' ') < 9:\n",
    "        for word in sms.text.split(' '):\n",
    "            return HAM if word == word.isupper() else ABSTAIN\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an external statistical model\n",
    "\n",
    "When labeling data, you can use external models whose response can be important information for deciding how to label an example. Snorkel has several built-in integrations in the form of the `Preprocessor` interface, in the example below we will use the `SpaCy` library to perform additional grammatical analysis of the text. However, you will need to download the English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:31:22.242830Z",
     "start_time": "2021-05-19T19:31:03.524220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in ./.venv/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in ./.venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:31:34.018309Z",
     "start_time": "2021-05-19T19:31:33.068241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.7.4) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation: /Users/dawid/Desktop/PUT/Data\n",
      "Mining/Data_Mining/.venv/lib/python3.12/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME             SPACY            VERSION                            \n",
      "en_core_web_sm   >=3.7.2,<3.8.0   \u001b[38;5;2m3.7.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:26.090920Z",
     "start_time": "2021-05-19T19:35:25.099419Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:26.870094Z",
     "start_time": "2021-05-19T19:35:26.810785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England GPE\n",
      "the United Kingdom GPE\n",
      "Wales ORG\n",
      "Scotland GPE\n",
      "The Irish Sea LOC\n",
      "England GPE\n",
      "the Celtic Sea LOC\n",
      "England GPE\n",
      "Europe LOC\n",
      "the North Sea LOC\n",
      "English LANGUAGE\n",
      "five-eighths CARDINAL\n",
      "Great Britain GPE\n",
      "the North Atlantic LOC\n",
      "over 100 CARDINAL\n",
      "the Isles of Scilly GPE\n",
      "the Isle of Wight GPE\n"
     ]
    }
   ],
   "source": [
    "_text = \"\"\"I don't England is a country that is part of the United Kingdom. \n",
    "It shares land borders with Wales to its west and Scotland to its north. \n",
    "The Irish Sea lies northwest of England and the Celtic Sea to the southwest. \n",
    "England is separated from continental Europe by the North Sea to the east and the \n",
    "English Channel to the south. The country covers five-eighths of the island of \n",
    "Great Britain, which lies in the North Atlantic, and includes over 100 smaller islands, \n",
    "such as the Isles of Scilly and the Isle of Wight.\"\"\"\n",
    "\n",
    "doc = nlp(_text)\n",
    "\n",
    "for e in doc.ents:\n",
    "    print(e.text, e.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:28.056838Z",
     "start_time": "2021-05-19T19:35:27.497633Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that short text messages in which a reference to a specific person appears are not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:28.564396Z",
     "start_time": "2021-05-19T19:35:28.552608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:29.352633Z",
     "start_time": "2021-05-19T19:35:29.348630Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function(pre=[spacy])\n",
    "def has_person(sms):\n",
    "    if len(sms.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in sms.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:09.731132Z",
     "start_time": "2021-05-19T19:35:29.949300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4458/4458 [00:22<00:00, 197.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.001346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.006281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_now</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_I_am_free</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.006281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_many_uppercase_words</th>\n",
       "      <td>4</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.180574</td>\n",
       "      <td>0.026694</td>\n",
       "      <td>0.007851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.047555</td>\n",
       "      <td>0.008973</td>\n",
       "      <td>0.008973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          j Polarity  Coverage  Overlaps  Conflicts\n",
       "check                     0      [1]  0.011216  0.003140   0.001346\n",
       "free                      1      [1]  0.045536  0.045536   0.006281\n",
       "call_now                  2      [1]  0.004486  0.004038   0.000000\n",
       "regex_I_am_free           3   [0, 1]  0.045536  0.045536   0.006281\n",
       "has_many_uppercase_words  4      [1]  0.180574  0.026694   0.007851\n",
       "has_person                5      [0]  0.047555  0.008973   0.008973"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words, has_person]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of pre-processing data for labeling would be determining the average word frequency of a document. Below we define a function that determines the average word frequency and we decorate it as an example of a pre-processor. When a text message is sent to the next labeling function, the pre-processor will populate the text message with the average word frequency and, based on that, the labeling function will make a decision (we assume that if the text message contains many rare words then it is spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:32.970893Z",
     "start_time": "2021-05-19T19:37:32.784823Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordfreq import zipf_frequency\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def avg_word_freq(sms):\n",
    "    sms.avg_word_freq = sum([zipf_frequency(word, 'en') for word in sms.text.split()]) / len(sms.text.split())\n",
    "    \n",
    "    return sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:34.795326Z",
     "start_time": "2021-05-19T19:37:34.790325Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function(pre=[avg_word_freq])\n",
    "def many_rare_words(sms):\n",
    "    return ABSTAIN if sms.avg_word_freq >= 4 else SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:39.197040Z",
     "start_time": "2021-05-19T19:37:35.984206Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4458/4458 [00:00<00:00, 6523.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>0.001346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.006281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_now</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_I_am_free</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.006281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_many_uppercase_words</th>\n",
       "      <td>4</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.180574</td>\n",
       "      <td>0.044639</td>\n",
       "      <td>0.007851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.047555</td>\n",
       "      <td>0.016375</td>\n",
       "      <td>0.016375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many_rare_words</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.061238</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>0.008748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          j Polarity  Coverage  Overlaps  Conflicts\n",
       "check                     0      [1]  0.011216  0.003589   0.001346\n",
       "free                      1      [1]  0.045536  0.045536   0.006281\n",
       "call_now                  2      [1]  0.004486  0.004038   0.000000\n",
       "regex_I_am_free           3   [0, 1]  0.045536  0.045536   0.006281\n",
       "has_many_uppercase_words  4      [1]  0.180574  0.044639   0.007851\n",
       "has_person                5      [0]  0.047555  0.016375   0.016375\n",
       "many_rare_words           6      [1]  0.061238  0.032526   0.008748"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words, has_person, many_rare_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:39.849049Z",
     "start_time": "2021-05-19T19:37:39.827079Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2775</th>\n",
       "      <td>Dude u knw also telugu..thts gud..k, gud nyt..</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870..k</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4808</th>\n",
       "      <td>PRIVATE! Your 2004 Account Statement for 078498****7 shows 786 unredeemed Bonus Points. To claim call 08719180219 Identifier Code: 45239 Expires 06.05.05</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3388</th>\n",
       "      <td>So u workin overtime nigpun?</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>U can WIN £100 of Music Gift Vouchers every week starting NOW Txt the word DRAW to 87066 TsCs www.Idew.com SkillGame, 1Winaweek, age16. 150ppermessSubscription</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>08714712388 between 10am-7pm Cost 10p</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3582</th>\n",
       "      <td>Ard 4 lor...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4572</th>\n",
       "      <td>CHA QUITEAMUZING THATSCOOL BABE,PROBPOP IN &amp; CU SATTHEN HUNNY 4BREKKIE! LOVE JEN XXX. PSXTRA LRG PORTIONS 4 ME PLEASE</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>+449071512431 URGENT! This is the 2nd attempt to contact U!U have WON £1250 CALL 09071512433 b4 050703 T&amp;CsBCM4235WC1N3XX. callcost 150ppm mobilesvary. max£7. 50</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>ALRITE</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>Somewhr someone is surely made 4 u. And God has decided a perfect time to make u meet dat person. . . . till den, . . . . . Enjoy ur crushes..!!!;-)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>Smith waste da.i wanna gayle.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>Babe ? I lost you ... :-(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>Near kalainar tv office.thenampet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>500 New Mobiles from 2004, MUST GO! Txt: NOKIA to No: 89545 &amp; collect yours today!From ONLY £1 www.4-tc.biz 2optout 087187262701.50gbp/mtmsg18 TXTAUCTION</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>Gud ni8.swt drms.take care</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Dear, will call Tmorrow.pls accomodate.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>Ok thanx...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>Hey sorry I didntgive ya a a bellearlier hunny,just been in bedbut mite go 2 thepub l8tr if uwana mt up?loads a luv Jenxxx.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>FREE for 1st week! No1 Nokia tone 4 ur mobile every week just txt NOKIA to 8077 Get txting and tell ur mates. www.getzed.co.uk POBox 36504 W45WQ 16+ norm150p/tone</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>Last Chance! Claim ur £150 worth of discount vouchers today! Text SHOP to 85023 now! SavaMob, offers mobile! T Cs SavaMob POBOX84, M263UZ. £3.00 Sub. 16</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3306</th>\n",
       "      <td>Ee msg na poortiyagi odalebeku: Hanumanji 7 name 1-Hanuman 2-Bajarangabali 3-Maruti 4-Pavanaputra 5-Sankatmochan 6-Ramaduth 7-Mahaveer ee 7 name  &amp;lt;#&amp;gt;  janarige ivatte kalisidare next saturday olage ondu good news keluviri...! Maretare inde 1 dodda problum nalli siguviri idu matra  &amp;lt;#&amp;gt; % true.. Don't neglet.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>K, wait chikku..il send aftr  &amp;lt;#&amp;gt; mins</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>FREE for 1st week! No1 Nokia tone 4 ur mob every week just txt NOKIA to 8007 Get txting and tell ur mates www.getzed.co.uk POBox 36504 W45WQ norm150p/tone 16+</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>No message..no responce..what happend?</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>Bugis oso near wat...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>Urgent! Please call 0906346330. Your ABTA complimentary 4* Spanish Holiday or £10,000 cash await collection SAE T&amp;Cs BOX 47 PO19 2EZ 150ppm 18+</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "2775                                                                                                                                                                                                                                                                                    Dude u knw also telugu..thts gud..k, gud nyt..   \n",
       "3167                                                                                                                                                                                                                             HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870..k   \n",
       "4808                                                                                                                                                                         PRIVATE! Your 2004 Account Statement for 078498****7 shows 786 unredeemed Bonus Points. To claim call 08719180219 Identifier Code: 45239 Expires 06.05.05   \n",
       "3388                                                                                                                                                                                                                                                                                                      So u workin overtime nigpun?   \n",
       "4237                                                                                                                                                                   U can WIN £100 of Music Gift Vouchers every week starting NOW Txt the word DRAW to 87066 TsCs www.Idew.com SkillGame, 1Winaweek, age16. 150ppermessSubscription   \n",
       "713                                                                                                                                                                                                                                                                                              08714712388 between 10am-7pm Cost 10p   \n",
       "3582                                                                                                                                                                                                                                                                                                                      Ard 4 lor...   \n",
       "4572                                                                                                                                                                                                           CHA QUITEAMUZING THATSCOOL BABE,PROBPOP IN & CU SATTHEN HUNNY 4BREKKIE! LOVE JEN XXX. PSXTRA LRG PORTIONS 4 ME PLEASE    \n",
       "717                                                                                                                                                                  +449071512431 URGENT! This is the 2nd attempt to contact U!U have WON £1250 CALL 09071512433 b4 050703 T&CsBCM4235WC1N3XX. callcost 150ppm mobilesvary. max£7. 50   \n",
       "2908                                                                                                                                                                                                                                                                                                                            ALRITE   \n",
       "1591                                                                                                                                                                              Somewhr someone is surely made 4 u. And God has decided a perfect time to make u meet dat person. . . . till den, . . . . . Enjoy ur crushes..!!!;-)   \n",
       "2867                                                                                                                                                                                                                                                                                                     Smith waste da.i wanna gayle.   \n",
       "1147                                                                                                                                                                                                                                                                                                         Babe ? I lost you ... :-(   \n",
       "1572                                                                                                                                                                                                                                                                                                 Near kalainar tv office.thenampet   \n",
       "1375                                                                                                                                                                         500 New Mobiles from 2004, MUST GO! Txt: NOKIA to No: 89545 & collect yours today!From ONLY £1 www.4-tc.biz 2optout 087187262701.50gbp/mtmsg18 TXTAUCTION   \n",
       "5265                                                                                                                                                                                                                                                                                                        Gud ni8.swt drms.take care   \n",
       "132                                                                                                                                                                                                                                                                                            Dear, will call Tmorrow.pls accomodate.   \n",
       "2842                                                                                                                                                                                                                                                                                                                       Ok thanx...   \n",
       "1267                                                                                                                                                                                                       Hey sorry I didntgive ya a a bellearlier hunny,just been in bedbut mite go 2 thepub l8tr if uwana mt up?loads a luv Jenxxx.   \n",
       "803                                                                                                                                                                 FREE for 1st week! No1 Nokia tone 4 ur mobile every week just txt NOKIA to 8077 Get txting and tell ur mates. www.getzed.co.uk POBox 36504 W45WQ 16+ norm150p/tone   \n",
       "2881                                                                                                                                                                          Last Chance! Claim ur £150 worth of discount vouchers today! Text SHOP to 85023 now! SavaMob, offers mobile! T Cs SavaMob POBOX84, M263UZ. £3.00 Sub. 16   \n",
       "3306  Ee msg na poortiyagi odalebeku: Hanumanji 7 name 1-Hanuman 2-Bajarangabali 3-Maruti 4-Pavanaputra 5-Sankatmochan 6-Ramaduth 7-Mahaveer ee 7 name  &lt;#&gt;  janarige ivatte kalisidare next saturday olage ondu good news keluviri...! Maretare inde 1 dodda problum nalli siguviri idu matra  &lt;#&gt; % true.. Don't neglet.   \n",
       "926                                                                                                                                                                                                                                                                                       K, wait chikku..il send aftr  &lt;#&gt; mins   \n",
       "1017                                                                                                                                                                    FREE for 1st week! No1 Nokia tone 4 ur mob every week just txt NOKIA to 8007 Get txting and tell ur mates www.getzed.co.uk POBox 36504 W45WQ norm150p/tone 16+   \n",
       "2387                                                                                                                                                                                                                                                                                            No message..no responce..what happend?   \n",
       "1351                                                                                                                                                                                                                                                                                                            Bugis oso near wat...    \n",
       "4183                                                                                                                                                                                   Urgent! Please call 0906346330. Your ABTA complimentary 4* Spanish Holiday or £10,000 cash await collection SAE T&Cs BOX 47 PO19 2EZ 150ppm 18+   \n",
       "\n",
       "      label  \n",
       "2775     -1  \n",
       "3167     -1  \n",
       "4808     -1  \n",
       "3388     -1  \n",
       "4237     -1  \n",
       "713      -1  \n",
       "3582     -1  \n",
       "4572     -1  \n",
       "717      -1  \n",
       "2908     -1  \n",
       "1591     -1  \n",
       "2867     -1  \n",
       "1147     -1  \n",
       "1572     -1  \n",
       "1375     -1  \n",
       "5265     -1  \n",
       "132      -1  \n",
       "2842     -1  \n",
       "1267     -1  \n",
       "803      -1  \n",
       "2881     -1  \n",
       "3306     -1  \n",
       "926      -1  \n",
       "1017     -1  \n",
       "2387     -1  \n",
       "1351     -1  \n",
       "4183     -1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[L_train[:,6] == SPAM].sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks messages containing more than 3 adjectives as spam. Use the SpaCy library for pre-processing. \n",
    "\n",
    "__Hint__: the following example shows how to read the part-of-speech label for each token from the message being analyzed. For information on all token properties recognized by SpaCy, see [API documentation](https://spacy.io/api/token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:41.894274Z",
     "start_time": "2021-05-19T19:37:41.111265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yetunde    PROPN      NNP        Yetunde   \n",
      ",          PUNCT      ,          ,         \n",
      "i          PRON       PRP        I         \n",
      "'m         AUX        VBP        be        \n",
      "sorry      ADJ        JJ         sorry     \n",
      "but        CCONJ      CC         but       \n",
      "moji       ADJ        JJ         moji      \n",
      "and        CCONJ      CC         and       \n",
      "i          PRON       PRP        I         \n",
      "seem       VERB       VBP        seem      \n",
      "too        ADV        RB         too       \n",
      "busy       ADJ        JJ         busy      \n",
      "to         PART       TO         to        \n",
      "be         AUX        VB         be        \n",
      "able       ADJ        JJ         able      \n",
      "to         PART       TO         to        \n",
      "go         VERB       VB         go        \n",
      "shopping   NOUN       NN         shopping  \n",
      ".          PUNCT      .          .         \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sms = \"Yetunde, i'm sorry but moji and i seem too busy to be able to go shopping.\"\n",
    "\n",
    "for token in nlp(sms):\n",
    "    print(f\"{token.text:<10} {token.pos_:<10} {token.tag_:<10} {token.lemma_:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_MODEL = spacy.load('en_core_web_sm')\n",
    "@labeling_function()\n",
    "def is_adjective(sms):\n",
    "    return SPAM if sum(1 for token in NLP_MODEL(sms) if token.pos_ == \"ADJ\") > 3 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining labeling functions into a single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of labeling functions is not to achieve individually large coverage. Labeling functions are inherently noisy and can make many individual errors. The true utility of labeling functions becomes apparent when multiple functions are combined to form a single model.\n",
    "\n",
    "We will first build a simple model based on majority voting, and then build a more complex model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:10.800140Z",
     "start_time": "2021-05-19T19:37:42.829275Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4458/4458 [00:00<00:00, 47087.85it/s]\n",
      "100%|██████████| 1114/1114 [00:06<00:00, 180.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.001346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.006281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_now</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_I_am_free</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.006281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.047555</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>0.010319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many_rare_words</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.061238</td>\n",
       "      <td>0.014581</td>\n",
       "      <td>0.008748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 j Polarity  Coverage  Overlaps  Conflicts\n",
       "check            0      [1]  0.011216  0.001795   0.001346\n",
       "free             1      [1]  0.045536  0.045536   0.006281\n",
       "call_now         2      [1]  0.004486  0.001795   0.000000\n",
       "regex_I_am_free  3   [0, 1]  0.045536  0.045536   0.006281\n",
       "has_person       4      [0]  0.047555  0.010319   0.010319\n",
       "many_rare_words  5      [1]  0.061238  0.014581   0.008748"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_person, many_rare_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:11.685278Z",
     "start_time": "2021-05-19T19:38:11.659189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.055655</td>\n",
       "      <td>0.055655</td>\n",
       "      <td>0.008977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call_now</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_I_am_free</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.055655</td>\n",
       "      <td>0.055655</td>\n",
       "      <td>0.008977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.057451</td>\n",
       "      <td>0.009874</td>\n",
       "      <td>0.009874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many_rare_words</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.071813</td>\n",
       "      <td>0.017056</td>\n",
       "      <td>0.009874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 j Polarity  Coverage  Overlaps  Conflicts\n",
       "check            0      [1]  0.008977  0.000000   0.000000\n",
       "free             1      [1]  0.055655  0.055655   0.008977\n",
       "call_now         2      [1]  0.001795  0.000000   0.000000\n",
       "regex_I_am_free  3   [0, 1]  0.055655  0.055655   0.008977\n",
       "has_person       4      [0]  0.057451  0.009874   0.009874\n",
       "many_rare_words  5      [1]  0.071813  0.017056   0.009874"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L=L_test, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:13.165490Z",
     "start_time": "2021-05-19T19:38:12.488067Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:14.046160Z",
     "start_time": "2021-05-19T19:38:14.041527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:14.971642Z",
     "start_time": "2021-05-19T19:38:14.958011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: -1, count: 3844\n",
      "LABEL: 0, count: 166\n",
      "LABEL: 1, count: 448\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels, counts = np.unique(preds_train, return_counts=True)\n",
    "\n",
    "for l, c in zip(labels, counts):\n",
    "    print(f\"LABEL: {l}, count: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:16.095821Z",
     "start_time": "2021-05-19T19:38:15.735200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|          | 0/500 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.009]\n",
      "INFO:root:[100 epochs]: TRAIN:[loss=0.003]\n",
      "INFO:root:[200 epochs]: TRAIN:[loss=0.003]\n",
      "INFO:root:[300 epochs]: TRAIN:[loss=0.002]\n",
      " 78%|███████▊  | 390/500 [00:00<00:00, 3893.39epoch/s]INFO:root:[400 epochs]: TRAIN:[loss=0.001]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3877.56epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:16.707091Z",
     "start_time": "2021-05-19T19:38:16.660020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting accuracy: 50.4%\n",
      "Probabilistic model accuracy: 50.9%\n"
     ]
    }
   ],
   "source": [
    "majority_acc = majority_model.score(L=L_test, Y=df_test.label, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Majority voting accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=df_test.label, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Probabilistic model accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, some data points will not receive any label. It is necessary to filter out these points before sending the labeling result for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:18.328146Z",
     "start_time": "2021-05-19T19:38:18.257149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4458, 2), (677, 2))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.utils import preds_to_probs, probs_to_preds\n",
    "\n",
    "preds_train, probs_train = label_model.predict(L=L_train, return_probs=True)\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(X=df_train, y=probs_train, L=L_train)\n",
    "df_train.shape, df_train_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we were able to quickly prepare labels for about 650 examples (recall that initially no example in the `df_train` set had labels).\n",
    "\n",
    "The next step will use prepared labels as training data for the actual classifier. We will use simple [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), first pre-processing the input data. Since we are working with text, we will use the [word vector representation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) created based on 5-grams by `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:20.290628Z",
     "start_time": "2021-05-19T19:38:20.140958Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.utils import probs_to_preds\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5))\n",
    "\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:21.403376Z",
     "start_time": "2021-05-19T19:38:21.027861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1000.0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=1000.0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1000.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression(C=1e3, solver='lbfgs')\n",
    "sklearn_model.fit(X=X_train, y=preds_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:22.426987Z",
     "start_time": "2021-05-19T19:38:22.421221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 53.3%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Logistic regression accuracy: {sklearn_model.score(X=X_test, y=df_test.label) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the final model improved the score over the majority vote and the `LabelModel` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Complete the above calls with functions that you wrote yourself and check whether your functions improve the quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "INFO:root:[0 epochs]: TRAIN:[loss=0.011]\n",
      "INFO:root:Finished Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Majority voting accuracy: 51.6%\n",
      "Probabilistic model accuracy: 52.2%\n",
      "Logistic regression accuracy: 46.9%\n"
     ]
    }
   ],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_person, many_rare_words, hot, contains_money, short_and_no_uppercase, ]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train, progress_bar=False)\n",
    "L_test = applier.apply(df=df_test, progress_bar=False)\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train)\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100000, seed=42, progress_bar=False)\n",
    "majority_acc = majority_model.score(L=L_test, Y=df_test.label, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"\\n{'Majority voting accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=df_test.label, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Probabilistic model accuracy:':<25} {label_model_acc * 100:.1f}%\")\n",
    "\n",
    "preds_train, probs_train = label_model.predict(L=L_train, return_probs=True)\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(X=df_train, y=probs_train, L=L_train)\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5))\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())\n",
    "sklearn_model = LogisticRegression(C=1e3, solver='lbfgs')\n",
    "sklearn_model.fit(X=X_train, y=preds_train_filtered)\n",
    "print(f\"Logistic regression accuracy: {sklearn_model.score(X=X_test, y=df_test.label) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming functions\n",
    "\n",
    "The idea of a transforming function is to perform an atomic transformation of an instance. For data that is an image, typical transformations include cropping, rotating, and changing the color palette. For text data, you can replace words with synonyms, substitute named entities, cut random pieces of text, etc. In the following example we will find types of named entities occurring in the text, and then prepare a simple transformer that will randomly replace occurrences of the `PERSON` entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:31.560232Z",
     "start_time": "2021-05-19T19:38:26.866544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('Sunshine Quiz Wkly', 'PERSON'), ('Sony', 'ORG'), ('Algarve', 'ORG'), ('82277', 'DATE'), ('1.50', 'MONEY')]\n",
      "Entities: [('07090298926', 'CARDINAL')]\n",
      "Entities: [('zyada kisi ko kuch nahi milta', 'PERSON'), ('Zindgi', 'PERSON'), ('Zindgi wo hai', 'GPE'), ('jo ham jeetey hai', 'PERSON')]\n",
      "Entities: [('2003', 'DATE'), ('07753741225', 'DATE'), ('800', 'CARDINAL'), ('un', 'ORG'), ('S. I. M.', 'PERSON'), ('42478', 'CARDINAL'), ('24/10/04', 'DATE')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('ALRITE SAM', 'ORG'), ('NIC', 'ORG')]\n",
      "Entities: [('Yar', 'PERSON'), ('ge', 'ORG')]\n",
      "Entities: [('2', 'CARDINAL'), ('One', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: [('Babe', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: [('a great month', 'DATE')]\n",
      "Entities: [('4', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('18 days', 'DATE'), ('daily', 'DATE')]\n",
      "Entities: [('Yavnt', 'NORP')]\n",
      "Entities: [('1', 'CARDINAL'), ('7', 'CARDINAL'), ('next saturday', 'DATE'), ('Maretare', 'ORG'), ('1', 'CARDINAL')]\n",
      "Entities: [('Gibbs', 'GPE')]\n",
      "Entities: [('Midnight', 'TIME')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('UK', 'GPE'), ('86688', 'DATE'), ('150p', 'CARDINAL')]\n",
      "Entities: [('Tmrw', 'PERSON'), ('9', 'CARDINAL')]\n",
      "Entities: [('Hype park plaza', 'FAC'), ('700', 'MONEY'), ('2 bedrm-$900...', 'QUANTITY')]\n",
      "Entities: [('Pete', 'PERSON'), ('Jenny', 'PERSON')]\n",
      "Entities: [('ALWAYS', 'ORG'), ('every day', 'DATE'), ('saturday', 'DATE')]\n",
      "Entities: []\n",
      "Entities: [('helens fone', 'PERSON'), ('2', 'CARDINAL'), ('Kate', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('BSLVYL', 'PERSON')]\n",
      "Entities: [('jay', 'PERSON')]\n",
      "Entities: [('today', 'DATE')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('1', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: [('saturday', 'DATE')]\n",
      "Entities: []\n",
      "Entities: [('Loooooool', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('1030', 'DATE')]\n",
      "Entities: []\n",
      "Entities: [('1', 'CARDINAL')]\n",
      "Entities: [('Finish', 'NORP')]\n",
      "Entities: [('URGENT', 'ORG'), ('U. Todays', 'ORG'), ('800', 'MONEY'), ('GUARANTEED', 'FAC')]\n",
      "Entities: [('italian', 'NORP')]\n",
      "Entities: []\n",
      "Entities: [('Sorry de', 'PERSON')]\n",
      "Entities: [('2.50', 'MONEY'), ('UNICEF', 'ORG'), ('Asian', 'NORP'), ('2.50', 'MONEY')]\n",
      "Entities: [('Shit', 'ORG'), ('this morning', 'TIME'), ('Another night', 'TIME')]\n",
      "Entities: [('1st', 'ORDINAL'), ('TONE', 'ORG'), ('20', 'CARDINAL'), ('every week', 'DATE'), ('just £1.50', 'MONEY'), ('2', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: [('one night', 'TIME')]\n",
      "Entities: [('1', 'CARDINAL'), ('2', 'CARDINAL'), ('5)Gently', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: [(\"The First Time In The History 'Need' 'Comfort' And 'Luxury' Are Sold\", 'ORG'), ('India', 'GPE'), ('Petrol-Rs', 'PERSON'), ('Beer-Rs', 'PERSON')]\n",
      "Entities: [('Uncle G', 'PERSON'), ('a rewarding month', 'DATE')]\n",
      "Entities: [('week', 'DATE'), ('09050002311', 'CARDINAL'), ('18', 'CARDINAL')]\n",
      "Entities: [('dec  &lt;#&gt', 'ORG'), ('a fantastic year', 'DATE'), ('Usmle', 'ORG')]\n",
      "Entities: [('BOO BABE', 'ORG'), ('2', 'CARDINAL'), ('CARE & I\\x92LLSPEAK 2U', 'ORG')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('yun dun wan juz', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('6', 'CARDINAL'), ('longer than an hour', 'TIME')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('Yar', 'PERSON')]\n",
      "Entities: [('a day', 'DATE'), ('Good days', 'DATE'), ('Bad days', 'DATE')]\n",
      "Entities: [('City Break', 'LOC'), ('WIN', 'ORG'), ('200', 'CARDINAL'), ('Summer Shopping', 'PERSON')]\n",
      "Entities: [('Christians', 'NORP')]\n",
      "Entities: []\n",
      "Entities: [('Xmas', 'PERSON')]\n",
      "Entities: [('4', 'CARDINAL'), ('today', 'DATE')]\n",
      "Entities: [('Congratulations - Thanks', 'ORG'), ('2,000', 'MONEY'), ('2', 'CARDINAL'), ('Only 10p', 'DATE'), ('BT', 'GPE')]\n",
      "Entities: [('northampton', 'GPE'), ('today', 'DATE'), ('ho ho', 'PERSON'), ('wednesday', 'DATE'), ('this week', 'DATE')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('Aka Egbon', 'PERSON')]\n",
      "Entities: [('Santa Calling', 'ORG'), ('Santa Xmas', 'GPE'), ('09058094583', 'DATE')]\n",
      "Entities: [('this weekend', 'DATE')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('bhaji', 'PERSON'), ('kallis', 'PERSON')]\n",
      "Entities: [('a week', 'DATE'), ('HAUGHAIGHGTUJHYGUJ', 'ORG')]\n",
      "Entities: []\n",
      "Entities: [('yan jiu', 'PERSON'), ('blah blah blah...', 'PERSON')]\n",
      "Entities: [('1 hour', 'TIME')]\n",
      "Entities: [('Twiggs St', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: [('Babe', 'PERSON')]\n",
      "Entities: [('4 christmas', 'DATE')]\n",
      "Entities: [('530', 'CARDINAL')]\n",
      "Entities: [('today', 'DATE')]\n",
      "Entities: [('dear.with', 'PERSON'), ('NRI', 'ORG')]\n",
      "Entities: [('you:-', 'GPE')]\n",
      "Entities: []\n",
      "Entities: [('Today', 'DATE'), ('150', 'MONEY'), ('85023', 'DATE'), ('SavaMob', 'ORG'), ('16', 'CARDINAL')]\n",
      "Entities: [('Nvm', 'PERSON')]\n",
      "Entities: [('4', 'CARDINAL')]\n",
      "Entities: [('83110', 'DATE')]\n",
      "Entities: [('Arngd', 'NORP'), ('snake &', 'ORG')]\n",
      "Entities: []\n",
      "Entities: [('taylor', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: [('13/4/04', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('That day', 'DATE'), ('paragon', 'ORG')]\n",
      "Entities: [('a gr8 day', 'DATE')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('Headin', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: [('tomorrow', 'DATE')]\n",
      "Entities: [('Nah', 'PERSON')]\n",
      "Entities: [('minutes', 'TIME'), ('Gyno', 'PERSON')]\n",
      "Entities: [('morning', 'TIME')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('K', 'ORG'), ('jason', 'PERSON')]\n",
      "Entities: [('frnd', 'ORG')]\n",
      "Entities: []\n",
      "Entities: [('Last weekends', 'DATE'), ('1000', 'MONEY'), ('GUARANTEED', 'GPE'), ('Code K52', 'PERSON'), ('12hrs', 'CARDINAL'), ('150ppm', 'CARDINAL')]\n",
      "Entities: [('2marrow', 'DATE'), ('2', 'CARDINAL')]\n",
      "Entities: [('Babe', 'PERSON')]\n",
      "Entities: [('3', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('4', 'CARDINAL')]\n",
      "Entities: [('Mila', 'PERSON'), ('UK', 'GPE'), ('UK', 'GPE'), ('69866.18', 'CARDINAL'), ('1.50', 'MONEY')]\n",
      "Entities: [('Kate', 'PERSON')]\n",
      "Entities: [('2', 'CARDINAL'), ('uwana', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('Moji', 'PERSON'), ('next friday', 'DATE')]\n",
      "Entities: [('al', 'PERSON')]\n",
      "Entities: [('yam cake', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('SUMMER', 'ORG'), ('OptOut', 'ORG')]\n",
      "Entities: [('4', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [(\"FR'NDSHIP\", 'PERSON'), ('2', 'CARDINAL'), ('itz', 'ORG'), ('only 4few seconds', 'TIME')]\n",
      "Entities: [('Xuhui', 'ORG')]\n",
      "Entities: [('BRAND', 'ORG'), ('Nokia', 'ORG'), ('7250', 'DATE'), ('4', 'CARDINAL'), ('today', 'DATE'), ('Txt NOKIA', 'PERSON'), ('86021', 'DATE')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('2', 'CARDINAL')]\n",
      "Entities: [('this year', 'DATE'), ('a great week', 'DATE')]\n",
      "Entities: [('Valentines Day Special!', 'ORG'), ('1000', 'MONEY'), ('83600', 'CARDINAL'), ('150p', 'CARDINAL')]\n",
      "Entities: [('kettoda manda', 'PERSON')]\n",
      "Entities: [('Bt', 'GPE'), ('2', 'CARDINAL'), ('about 2', 'CARDINAL'), ('Boost', 'PERSON'), ('Thy', 'PERSON'), ('2gthr', 'CARDINAL')]\n",
      "Entities: [('NTT Ltd', 'ORG'), ('PO Box', 'ORG'), ('1327', 'DATE'), ('Croydon CR9 5WB 0870', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: [('1', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('1st', 'ORDINAL'), ('2', 'CARDINAL'), ('Classic Nokia', 'PRODUCT'), ('Polys', 'PERSON'), ('16+', 'DATE')]\n",
      "Entities: [('10', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('2003', 'DATE'), ('800', 'CARDINAL'), ('un', 'ORG'), ('S. I. M.', 'PERSON'), ('48922', 'DATE'), ('21/11/04', 'CARDINAL')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('Arabian', 'NORP'), ('Mmmmmm', 'NORP'), ('Yummy', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('Nah', 'PERSON'), ('Wednesday', 'DATE')]\n",
      "Entities: [('First', 'ORDINAL')]\n",
      "Entities: [('4', 'CARDINAL'), ('PUTTIN', 'ORG'), ('FONE', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('IM', 'GPE'), ('XXX', 'PERSON')]\n",
      "Entities: [('4', 'CARDINAL'), ('Best Life', 'WORK_OF_ART')]\n",
      "Entities: [('gotto www.comuk.net', 'PERSON'), ('3qxj9', 'CARDINAL'), ('08702840625', 'DATE'), ('9AE', 'GPE')]\n",
      "Entities: [('last night', 'TIME'), ('Patrick Swayze', 'PERSON')]\n",
      "Entities: []\n",
      "Entities: [('4', 'CARDINAL')]\n",
      "Entities: [('Helloooo', 'PERSON'), ('Sweet', 'PERSON'), ('morning', 'TIME')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('CALM DOWNON', 'PERSON'), ('ITXT U COS IWANA', 'ORG'), ('THEW', 'ORG'), ('AGES', 'DATE')]\n",
      "Entities: []\n",
      "Entities: []\n",
      "Entities: [('Sinco', 'GPE'), ('Payee', 'GPE'), ('URN', 'GPE'), ('URN', 'ORG')]\n",
      "Entities: [('today', 'DATE')]\n",
      "Entities: [('Piggy', 'GPE'), ('4 lunch', 'QUANTITY')]\n",
      "Entities: [('7', 'CARDINAL'), ('6th', 'ORDINAL'), ('5th', 'ORDINAL'), ('Ur Lovely Friendship', 'WORK_OF_ART')]\n",
      "Entities: [('Abiola', 'ORG')]\n",
      "Entities: [('last week', 'DATE')]\n",
      "Entities: [('this morning', 'TIME')]\n",
      "Entities: []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for doc in nlp.pipe(df_train.text.sample(frac=0.05)):\n",
    "    print(f\"Entities: {[(e.text, e.label_) for e in doc.ents]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:40:36.945027Z",
     "start_time": "2021-05-19T19:39:12.256235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jurong point',\n",
       " 'Nah',\n",
       " 'Melle Melle',\n",
       " 'Mark',\n",
       " 'Yummy',\n",
       " 'Mark',\n",
       " 'Mallika Sherawat',\n",
       " 'Divorce Barbie',\n",
       " 'Ken',\n",
       " 'Wah']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_entities = []\n",
    "\n",
    "for doc in nlp.pipe(df_train.text):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'PERSON':\n",
    "            person_entities.append(e.text)\n",
    "        \n",
    "person_entities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:03.662025Z",
     "start_time": "2021-05-19T19:42:02.919683Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.augmentation import transformation_function\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def random_person_ner(sms):\n",
    "    person_ners = [e.text for e in sms.doc.ents]\n",
    "    \n",
    "    if person_ners:\n",
    "        person_to_replace = np.random.choice(person_ners)\n",
    "        person_to_add = np.random.choice(person_entities)\n",
    "        sms.text = sms.text.replace(person_to_replace, person_to_add)\n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of transformation could be using WordNet to find synonyms for words. However, this requires downloading a corpus of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:20.633409Z",
     "start_time": "2021-05-19T19:42:20.093443Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:25.058494Z",
     "start_time": "2021-05-19T19:42:25.054333Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_synonym(word):\n",
    "    \n",
    "    synsets = wordnet.synsets(word)\n",
    "    \n",
    "    if synsets:\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        \n",
    "        return np.random.choice([w.replace(\"_\", \" \") for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:27.153320Z",
     "start_time": "2021-05-19T19:42:27.148396Z"
    }
   },
   "outputs": [],
   "source": [
    "@transformation_function()\n",
    "def replace_words_with_synonym(sms, num_replacements=5):\n",
    "\n",
    "    words = sms.text.split()\n",
    "    \n",
    "    for _ in range(num_replacements):\n",
    "        word_idx = np.random.choice(range(len(words)))\n",
    "        synonym = get_synonym(words[word_idx])\n",
    "        if synonym:\n",
    "            words[word_idx] = synonym\n",
    "        \n",
    "    sms.text = ' '.join(words)\n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compare the original text message content with the transformed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:31.366022Z",
     "start_time": "2021-05-19T19:42:31.361290Z"
    }
   },
   "outputs": [],
   "source": [
    "# source: https://github.com/snorkel-team/snorkel-tutorials/blob/master/spam/utils.py\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def preview_tfs(df, tfs):\n",
    "    transformed_examples = []\n",
    "    for f in tfs:\n",
    "        for i, row in df.iterrows():\n",
    "            transformed_or_none = f(row)\n",
    "            # If TF returned a transformed example, record it in dict and move to next TF.\n",
    "            if transformed_or_none is not None:\n",
    "                transformed_examples.append(\n",
    "                    OrderedDict(\n",
    "                        {\n",
    "                            \"TF Name\": f.name,\n",
    "                            \"Original Text\": row.text,\n",
    "                            \"Transformed Text\": transformed_or_none.text,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "    return pd.DataFrame(transformed_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:46:55.348701Z",
     "start_time": "2021-05-19T19:46:54.714091Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/dawid/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/share/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/dawid/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/share/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tfs \u001b[38;5;241m=\u001b[39m [random_person_ner, replace_words_with_synonym]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpreview_tfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m, in \u001b[0;36mpreview_tfs\u001b[0;34m(df, tfs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m tfs:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m----> 9\u001b[0m         transformed_or_none \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# If TF returned a transformed example, record it in dict and move to next TF.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m transformed_or_none \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/snorkel/map/core.py:163\u001b[0m, in \u001b[0;36mBaseMapper.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mapper \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre:\n\u001b[1;32m    162\u001b[0m     x_mapped \u001b[38;5;241m=\u001b[39m mapper(x_mapped)\n\u001b[0;32m--> 163\u001b[0m x_mapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_mapped_data_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[x_hashable] \u001b[38;5;241m=\u001b[39m x_mapped\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/snorkel/map/core.py:321\u001b[0m, in \u001b[0;36mLambdaMapper._generate_mapped_data_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_mapped_data_point\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: DataPoint) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[DataPoint]:\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 8\u001b[0m, in \u001b[0;36mreplace_words_with_synonym\u001b[0;34m(sms, num_replacements)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_replacements):\n\u001b[1;32m      7\u001b[0m     word_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words)))\n\u001b[0;32m----> 8\u001b[0m     synonym \u001b[38;5;241m=\u001b[39m \u001b[43mget_synonym\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m synonym:\n\u001b[1;32m     10\u001b[0m         words[word_idx] \u001b[38;5;241m=\u001b[39m synonym\n",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m, in \u001b[0;36mget_synonym\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_synonym\u001b[39m(word):\n\u001b[0;32m----> 3\u001b[0m     synsets \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynsets\u001b[49m(word)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m synsets:\n\u001b[1;32m      6\u001b[0m         words \u001b[38;5;241m=\u001b[39m [lemma\u001b[38;5;241m.\u001b[39mname() \u001b[38;5;28;01mfor\u001b[39;00m lemma \u001b[38;5;129;01min\u001b[39;00m synsets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlemmas()]\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/dawid/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/share/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tfs = [random_person_ner, replace_words_with_synonym]\n",
    "\n",
    "preview_tfs(df_train.sample(frac=0.1), tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying transforming functions requires some policy defining the order and number of transformations. In the example below, two transformation functions are drawn at random and this sequence of two functions is applied twice to each data point. As a result, we triple the size of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:48:27.795346Z",
     "start_time": "2021-05-19T19:48:18.367019Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/446 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/dawid/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/share/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/dawid/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/share/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m tf_applier \u001b[38;5;241m=\u001b[39m PandasTFApplier(tfs, random_policy)\n\u001b[1;32m      7\u001b[0m df_train_sample \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m df_train_augmented \u001b[38;5;241m=\u001b[39m \u001b[43mtf_applier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_sample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/snorkel/augmentation/apply/pandas.py:64\u001b[0m, in \u001b[0;36mPandasTFApplier.apply\u001b[0;34m(self, df, progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m x_transformed: List[pd\u001b[38;5;241m.\u001b[39mSeries] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, x \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df), disable\u001b[38;5;241m=\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m progress_bar)):\n\u001b[0;32m---> 64\u001b[0m     x_transformed\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_policy_to_data_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(x_transformed, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39minfer_objects()\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/snorkel/augmentation/apply/core.py:47\u001b[0m, in \u001b[0;36mBaseTFApplier._apply_policy_to_data_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tf_idx \u001b[38;5;129;01min\u001b[39;00m seq:\n\u001b[1;32m     46\u001b[0m     tf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfs[tf_idx]\n\u001b[0;32m---> 47\u001b[0m     x_t_or_none \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Update if transformation was applied\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x_t_or_none \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/snorkel/map/core.py:163\u001b[0m, in \u001b[0;36mBaseMapper.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mapper \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre:\n\u001b[1;32m    162\u001b[0m     x_mapped \u001b[38;5;241m=\u001b[39m mapper(x_mapped)\n\u001b[0;32m--> 163\u001b[0m x_mapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_mapped_data_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[x_hashable] \u001b[38;5;241m=\u001b[39m x_mapped\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/snorkel/map/core.py:321\u001b[0m, in \u001b[0;36mLambdaMapper._generate_mapped_data_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_mapped_data_point\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: DataPoint) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[DataPoint]:\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 8\u001b[0m, in \u001b[0;36mreplace_words_with_synonym\u001b[0;34m(sms, num_replacements)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_replacements):\n\u001b[1;32m      7\u001b[0m     word_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words)))\n\u001b[0;32m----> 8\u001b[0m     synonym \u001b[38;5;241m=\u001b[39m \u001b[43mget_synonym\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m synonym:\n\u001b[1;32m     10\u001b[0m         words[word_idx] \u001b[38;5;241m=\u001b[39m synonym\n",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m, in \u001b[0;36mget_synonym\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_synonym\u001b[39m(word):\n\u001b[0;32m----> 3\u001b[0m     synsets \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynsets\u001b[49m(word)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m synsets:\n\u001b[1;32m      6\u001b[0m         words \u001b[38;5;241m=\u001b[39m [lemma\u001b[38;5;241m.\u001b[39mname() \u001b[38;5;28;01mfor\u001b[39;00m lemma \u001b[38;5;129;01min\u001b[39;00m synsets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlemmas()]\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/dawid/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/share/nltk_data'\n    - '/Users/dawid/Desktop/PUT/Data Mining/Data_Mining/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from snorkel.augmentation import RandomPolicy, PandasTFApplier\n",
    "\n",
    "random_policy = RandomPolicy(len(tfs), sequence_length=2, n_per_original=2, keep_original=True)\n",
    "\n",
    "tf_applier = PandasTFApplier(tfs, random_policy)\n",
    "\n",
    "df_train_sample = df_train.sample(frac=0.1)\n",
    "df_train_augmented = tf_applier.apply(df_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:48:30.188103Z",
     "start_time": "2021-05-19T19:48:30.181349Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train_augmented' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_train_sample\u001b[38;5;241m.\u001b[39mshape, \u001b[43mdf_train_augmented\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train_augmented' is not defined"
     ]
    }
   ],
   "source": [
    "df_train_sample.shape, df_train_augmented.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Modify the transforming function ``replace_words_with_synonym()`` so that you can restrict the replacement of words with synonyms only for specific parts of speech (e.g., replace only nouns or verbs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "NLP_MODEL = spacy.load('en_core_web_sm')\n",
    "@transformation_function()\n",
    "def replace_words_with_synonym(sms, num_replacements=5, word_type=None):\n",
    "\n",
    "    words = list(NLP_MODEL(sms.text))\n",
    "    \n",
    "    replacement_counter = 0\n",
    "    for ind, word in enumerate(words):\n",
    "        if replacement_counter >= num_replacements:\n",
    "            break\n",
    "        if word_type is None or word.pos_ != word_type:\n",
    "            continue\n",
    "        synonym = get_synonym(word.text)\n",
    "        if synonym:\n",
    "            words[ind] = synonym\n",
    "            replacement_counter += 1\n",
    "    else:\n",
    "        print(\"Not enough replacements found\")\n",
    "        # I know it's not neccessary to display this, but I wanted to use the else statement after loop, and this is perfect place for it\n",
    "    sms.text = ' '.join(word.text for word in words)\n",
    "    return sms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
